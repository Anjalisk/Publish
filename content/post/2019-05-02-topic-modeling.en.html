---
title: Topic Modeling
author: Anjali Khushalani
date: '2019-05-02'
slug: topic-modeling
categories:
  - R
tags:
  - R Markdown
description: ''
featured: pic05.jpeg
featuredalt: Pic 5
featuredpath: date
linktitle: ''
---



<div id="what-is-topic-modeling" class="section level2">
<h2>What is topic modeling?</h2>
<p>Topic modeling provides an algorithmic solution to managing, organizing and annotating large archival text. The annotations aid you in tasks of information retrieval, classification and corpus exploration</p>
</div>
<div id="setting-up-the-platform-to-perform-topic-modeling" class="section level2">
<h2>Setting up the platform to perform topic modeling</h2>
<p>You may want to consider installing these libraries using the install.packages(“packagename”) function and then calling the library</p>
<pre class="r"><code>library(wordcloud)
library(data.table)
library(tidytext)
library(dplyr)
library(ggplot2)
library(SnowballC)
library(RTextTools)
library(tm)
library(wordcloud)
library(topicmodels)
library(slam)
library(reshape2)
library(ldatuning)</code></pre>
</div>
<div id="data" class="section level2">
<h2>Data</h2>
<p>The data for this blog has been collected from psychcentral.com. This website offers an online forum for posting questions and answers related to mental health. Please visit <a href="https://forums.psychcentral.com" class="uri">https://forums.psychcentral.com</a> for more information. Our objective is to perform text analytics to discover useful information related to mental health.</p>
<pre class="r"><code>data &lt;- fread(&quot;/Users/Anjalikhushalani/Desktop/Publish/content/data/psychcentral_data.csv&quot;, sep=&quot;,&quot;, header=T, 
              strip.white = T, na.strings = c(&quot;NA&quot;,&quot;NaN&quot;,&quot;&quot;,&quot;?&quot;)) </code></pre>
</div>
<div id="using-libraries-dplyr-and-tidytext-to-tokenize-column-q_content.-then-remove-the-stop-words.-the-count-the-number-of-tokens." class="section level1">
<h1>Using libraries “dplyr” and “tidytext” to tokenize column q_content. Then remove the stop-words. The count the number of tokens.</h1>
<pre class="r"><code>#Tokenzing

#unnest_tokens above splits each row such that there is one token (word) in each row of the new data frame
tidy_text &lt;- data %&gt;%
  unnest_tokens(word, q_content)

tidy_text[1:10]</code></pre>
<pre><code>##     row              q_subject answers     word
##  1:   0 Saying Goodbye For Now    &lt;NA&gt;     nits
##  2:   0 Saying Goodbye For Now    &lt;NA&gt;     been
##  3:   0 Saying Goodbye For Now    &lt;NA&gt;        a
##  4:   0 Saying Goodbye For Now    &lt;NA&gt; pleasure
##  5:   0 Saying Goodbye For Now    &lt;NA&gt;  hosting
##  6:   0 Saying Goodbye For Now    &lt;NA&gt;     this
##  7:   0 Saying Goodbye For Now    &lt;NA&gt;  service
##  8:   0 Saying Goodbye For Now    &lt;NA&gt;     over
##  9:   0 Saying Goodbye For Now    &lt;NA&gt;      the
## 10:   0 Saying Goodbye For Now    &lt;NA&gt;     past</code></pre>
<pre class="r"><code>#Remove StopWords
data(stop_words)

tidy_text &lt;- tidy_text %&gt;%
  anti_join(stop_words)</code></pre>
<pre><code>## Joining, by = &quot;word&quot;</code></pre>
<pre class="r"><code>#Count
tidy_text %&gt;%
  count(word, sort = TRUE)</code></pre>
<pre><code>## # A tibble: 46,081 x 2
##    word        n
##    &lt;chr&gt;   &lt;int&gt;
##  1 im      13012
##  2 dont    11197
##  3 feel     9168
##  4 time     6697
##  5 life     4464
##  6 ive      4403
##  7 people   4233
##  8 told     4150
##  9 friends  4045
## 10 love     3281
## # … with 46,071 more rows</code></pre>
<pre class="r"><code>#Visualization that shows the frequency of the tokens that appeared for at least 2000 times. 
tidy_text %&gt;%
  count(word, sort = TRUE) %&gt;%
  filter(n &gt; 2000) %&gt;%
  mutate(word = reorder(word, n)) %&gt;%
  ggplot(aes(word, n)) +
  geom_bar(stat = &quot;identity&quot;) +
  xlab(NULL) +
  coord_flip()</code></pre>
<p><img src="/post/2019-05-02-topic-modeling.en_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<div id="now-using-snowball" class="section level2">
<h2>Now using snowball</h2>
</div>
</div>
<div id="what-is-snowball" class="section level1">
<h1>What is Snowball?</h1>
<p>An R interface to the C ‘libstemmer’ library that implements Porter’s word stemming algorithm for collapsing words to a common root to aid comparison of vocabulary. Currently supported languages are Danish, Dutch, English, Finnish, French, German, Hungarian, Italian, Norwegian, Portuguese, Romanian, Russian, Spanish, Swedish and Turkish.</p>
<pre class="r"><code>#Stemming using SnowballC
tidy_text &lt;- data %&gt;%
  unnest_tokens(word, q_content) %&gt;%
  mutate(word = wordStem(word))

#Removing stopwords
tidy_text &lt;- tidy_text %&gt;% anti_join(stop_words)</code></pre>
<pre><code>## Joining, by = &quot;word&quot;</code></pre>
<pre class="r"><code>#Count
tidy_text %&gt;%
  count(word, sort = TRUE)</code></pre>
<pre><code>## # A tibble: 36,177 x 2
##    word       n
##    &lt;chr&gt;  &lt;int&gt;
##  1 im     13013
##  2 feel   12905
##  3 dont   11197
##  4 time    8755
##  5 becaus  8106
##  6 realli  6780
##  7 friend  6417
##  8 talk    5314
##  9 veri    5234
## 10 tri     5183
## # … with 36,167 more rows</code></pre>
<pre class="r"><code>#Plot a visualization that shows the frequency of the tokens that appeared for at least 4000 times.
tidy_text %&gt;%
  count(word, sort = TRUE) %&gt;%
  filter(n &gt; 4000) %&gt;%
  mutate(word = reorder(word, n)) %&gt;%
  ggplot(aes(word, n)) +
  geom_bar(stat = &quot;identity&quot;) +
  xlab(NULL) +
  coord_flip()</code></pre>
<p><img src="/post/2019-05-02-topic-modeling.en_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<pre class="r"><code>#word cloud
tidy_text %&gt;%
  count(word) %&gt;%
  with(wordcloud(word, n, max.words = 200))</code></pre>
<p><img src="/post/2019-05-02-topic-modeling.en_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<pre class="r"><code>#colorcoded word cloud
tidy_text %&gt;%
  inner_join(get_sentiments(&quot;bing&quot;)) %&gt;%
  count(word, sentiment, sort = TRUE) %&gt;%
  acast(word ~ sentiment, value.var = &quot;n&quot;, fill = 0) %&gt;%
  comparison.cloud(colors = c(&quot;#0000FF&quot;, &quot;#FF0000&quot;),
                   max.words = 100)</code></pre>
<pre><code>## Joining, by = &quot;word&quot;</code></pre>
<p><img src="/post/2019-05-02-topic-modeling.en_files/figure-html/unnamed-chunk-5-2.png" width="672" /></p>
<div id="topic-modeling-using-lda-on-q-content-column" class="section level2">
<h2>Topic Modeling using LDA on “q-content” column</h2>
<pre class="r"><code># We perform LDA on the rows 1 through 1000 in the data
data &lt;- data[1:1000,]

#Removing these words based on the above result
wordstoremove &lt;- c(&quot;know&quot;,&quot;like&quot;,&quot;want&quot;,&quot;dont&quot;,
                   &quot;just&quot;,&quot;feel&quot;,&quot;get&quot;,&quot;really&quot;,&quot;get&quot;,&quot;said&quot;,&quot;time&quot;,
                   &quot;told&quot;,&quot;can&quot;,&quot;one&quot;,&quot;now&quot;,&quot;always&quot;,&quot;ive&quot;,&quot;even&quot;)

data_sub &lt;- as.data.frame(sapply(data, function(x) gsub(paste(wordstoremove, collapse = &#39;|&#39;), &#39;&#39;, x)))

corpus &lt;- Corpus(VectorSource(data_sub$q_content), readerControl=list(language=&quot;en&quot;))
dtm &lt;- DocumentTermMatrix(corpus, control = list(stopwords = TRUE, minWordLength = 2, removeNumbers = TRUE, removePunctuation = TRUE,  stemDocument = TRUE))

#Find the sum of words in each Document
rowTotals &lt;- apply(dtm , 1, sum) 

#remove all docs without words
dtm.new   &lt;- dtm[rowTotals&gt; 0, ] 

# k is the number of topics to be found.
lda &lt;- LDA(dtm.new, k = 5) 

lda_td &lt;- tidy(lda)</code></pre>
<p><strong>Visualization to find the 10 terms that are most common withineach topic using k =5</strong></p>
<pre class="r"><code>top_terms &lt;- lda_td %&gt;%
  group_by(topic) %&gt;%
  top_n(10, beta) %&gt;%
  ungroup() %&gt;%
  arrange(topic, -beta)

top_terms %&gt;%
  mutate(term = reorder(term, beta)) %&gt;%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_bar(stat = &quot;identity&quot;, show.legend = FALSE) +
  facet_wrap(~ topic, scales = &quot;free&quot;) +
  coord_flip()</code></pre>
<p><img src="/post/2019-05-02-topic-modeling.en_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
</div>
<div id="determine-k-by-tuning" class="section level2">
<h2>Determine k by tuning</h2>
<pre class="r"><code>#lda &lt;- LDA(dtm.new, k = 5)

tunes &lt;- FindTopicsNumber(
  dtm.new,
  topics = c(2:20),
  metrics = c(&quot;Griffiths2004&quot;, &quot;CaoJuan2009&quot;, &quot;Arun2010&quot;, &quot;Deveaud2014&quot;),
  method = &quot;Gibbs&quot;,
  control = list(seed = 77),
  mc.cores = 3L,
  verbose = TRUE
)</code></pre>
<pre><code>## fit models... done.
## calculate metrics:
##   Griffiths2004... done.
##   CaoJuan2009... done.
##   Arun2010... done.
##   Deveaud2014... done.</code></pre>
<pre class="r"><code>FindTopicsNumber_plot(tunes)</code></pre>
<p><img src="/post/2019-05-02-topic-modeling.en_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
</div>
<div id="experimenting-with-different-values-of-k" class="section level2">
<h2>Experimenting with different values of “k”</h2>
<p><strong>k=2</strong></p>
<pre class="r"><code>lda &lt;- LDA(dtm.new, k = 2) 

#Using the tidytext package for extracting the per-topic-per-word probabilities
lda_td &lt;- tidy(lda)
lda_td</code></pre>
<pre><code>## # A tibble: 28,048 x 3
##    topic term          beta
##    &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt;
##  1     1 aboven   0.0000331
##  2     2 aboven   0.0000133
##  3     1 account  0.0000782
##  4     2 account  0.000230 
##  5     1 actually 0.00258  
##  6     2 actually 0.0000411
##  7     1 advice   0.000129 
##  8     2 advice   0.00149  
##  9     1 already  0.000453 
## 10     2 already  0.000888 
## # … with 28,038 more rows</code></pre>
<pre class="r"><code>#Visualization to find the 10 terms that are most common within each topic
top_terms &lt;- lda_td %&gt;%
  group_by(topic) %&gt;%
  top_n(10, beta) %&gt;%
  ungroup() %&gt;%
  arrange(topic, -beta)

top_terms %&gt;%
  mutate(term = reorder(term, beta)) %&gt;%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_bar(stat = &quot;identity&quot;, show.legend = FALSE) +
  facet_wrap(~ topic, scales = &quot;free&quot;) +
  coord_flip()</code></pre>
<p><img src="/post/2019-05-02-topic-modeling.en_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p><strong>k= 3</strong></p>
<pre class="r"><code>lda &lt;- LDA(dtm.new, k = 3) 

#Using the tidytext package for extracting the per-topic-per-word probabilities
lda_td &lt;- tidy(lda)
lda_td</code></pre>
<pre><code>## # A tibble: 42,072 x 3
##    topic term          beta
##    &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt;
##  1     1 aboven   0.0000262
##  2     2 aboven   0.0000204
##  3     3 aboven   0.0000236
##  4     1 account  0.000249 
##  5     2 account  0.000123 
##  6     3 account  0.000105 
##  7     1 actually 0.00129  
##  8     2 actually 0.000954 
##  9     3 actually 0.00173  
## 10     1 advice   0.000605 
## # … with 42,062 more rows</code></pre>
<pre class="r"><code>#Visualization to find the 10 terms that are most common within each topic
top_terms &lt;- lda_td %&gt;%
  group_by(topic) %&gt;%
  top_n(10, beta) %&gt;%
  ungroup() %&gt;%
  arrange(topic, -beta)

top_terms %&gt;%
  mutate(term = reorder(term, beta)) %&gt;%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_bar(stat = &quot;identity&quot;, show.legend = FALSE) +
  facet_wrap(~ topic, scales = &quot;free&quot;) +
  coord_flip()</code></pre>
<p><img src="/post/2019-05-02-topic-modeling.en_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p><strong>k=4</strong></p>
<pre class="r"><code>lda &lt;- LDA(dtm.new, k = 4)

#Using the tidytext package for extracting the per-topic-per-word probabilities
lda_td &lt;- tidy(lda)
lda_td</code></pre>
<pre><code>## # A tibble: 56,096 x 3
##    topic term          beta
##    &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt;
##  1     1 aboven   3.56e-106
##  2     2 aboven   6.41e-  5
##  3     3 aboven   3.47e-  5
##  4     4 aboven   2.33e- 14
##  5     1 account  4.08e-  4
##  6     2 account  3.94e-  5
##  7     3 account  1.34e-  4
##  8     4 account  2.15e-  6
##  9     1 actually 7.81e-  4
## 10     2 actually 1.87e-  3
## # … with 56,086 more rows</code></pre>
<pre class="r"><code>#Visualization to find the 10 terms that are most common within each topic
top_terms &lt;- lda_td %&gt;%
  group_by(topic) %&gt;%
  top_n(10, beta) %&gt;%
  ungroup() %&gt;%
  arrange(topic, -beta)

top_terms %&gt;%
  mutate(term = reorder(term, beta)) %&gt;%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_bar(stat = &quot;identity&quot;, show.legend = FALSE) +
  facet_wrap(~ topic, scales = &quot;free&quot;) +
  coord_flip()</code></pre>
<p><img src="/post/2019-05-02-topic-modeling.en_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p><strong>k =10</strong></p>
<pre class="r"><code>lda &lt;- LDA(dtm.new, k = 10) # k is the number of topics to be found.

#Using the tidytext package for extracting the per-topic-per-word probabilities
lda_td &lt;- tidy(lda)
lda_td</code></pre>
<pre><code>## # A tibble: 140,240 x 3
##    topic term        beta
##    &lt;int&gt; &lt;chr&gt;      &lt;dbl&gt;
##  1     1 aboven 3.26e-189
##  2     2 aboven 8.08e-  5
##  3     3 aboven 1.50e-191
##  4     4 aboven 3.16e-191
##  5     5 aboven 6.92e-193
##  6     6 aboven 8.12e-  5
##  7     7 aboven 1.62e-191
##  8     8 aboven 1.52e-193
##  9     9 aboven 7.64e-  5
## 10    10 aboven 2.56e-195
## # … with 140,230 more rows</code></pre>
<pre class="r"><code>#Visualization to find the 10 terms that are most common within each topic
top_terms &lt;- lda_td %&gt;%
  group_by(topic) %&gt;%
  top_n(10, beta) %&gt;%
  ungroup() %&gt;%
  arrange(topic, -beta)

top_terms %&gt;%
  mutate(term = reorder(term, beta)) %&gt;%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_bar(stat = &quot;identity&quot;, show.legend = FALSE) +
  facet_wrap(~ topic, scales = &quot;free&quot;) +
  coord_flip()</code></pre>
<p><img src="/post/2019-05-02-topic-modeling.en_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>On the similar grounds topic modeling can performed on the “answers” column in the dataset.</p>
<p><strong>Conclusion</strong></p>
<p>Based on the above analysis : from word cloud there is typical pattern around questions, so questions have negative expressions around – depression, addiction, suffer, death, smoke, kill, lack of trust, fear, fail while the positive expression are love, respect, luck, support, fun, honest, patience, so this makes me come to a hypothesis that people suffer from mental disorders due to lack of support, love, trust and fear of failure.</p>
<p>The top topics based on topic modeling are people expressing concerns with regards to people seeking help from therapists and such platforms for the trouble around relationship, support, addiction, depression and determining the steps to maintain work life balance and being happy.</p>
</div>
</div>
