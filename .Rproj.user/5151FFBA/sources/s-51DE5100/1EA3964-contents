setwd("/Volumes/GoogleDrive/My Drive/2.ABA/Assignment#2")

library(data.table)
library(tidytext)
library(dplyr)
library(ggplot2)
library(SnowballC)
library(RTextTools)
library(tm)
library(wordcloud)
library(topicmodels)
library(slam)

data <- fread("psychcentral_data.csv", sep=",", header=T, 
              strip.white = T, na.strings = c("NA","NaN","","?")) 

#stemming - punctuation - stopwords
##Tokenzing

#unnest_tokens above splits each row such that 
#there is one token (word) in each row of the new data frame

tidy_text <- data %>%
  unnest_tokens(word, q_content)

tidy_text[1:20]

##Remove StopWords

data(stop_words)

tidy_text <- tidy_text %>%
  anti_join(stop_words)

##Count

tidy_text %>%
  count(word, sort = TRUE)

#Visualization that shows the frequency of the tokens that 
#appeared for at least 2000 times. 

tidy_text %>%
  count(word, sort = TRUE) %>%
  filter(n > 2000) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_bar(stat = "identity") +
  xlab(NULL) +
  coord_flip()

#Stemming using SnowballC

tidy_text <- data %>%
  unnest_tokens(word, q_content) %>%
  mutate(word = wordStem(word))

#Removing stopwords
tidy_text <- tidy_text %>% anti_join(stop_words)

##Count

tidy_text %>%
  count(word, sort = TRUE)

#Plot a visualization that shows the frequency of the 
#tokens that appeared for at least 4000 times.

tidy_text %>%
  count(word, sort = TRUE) %>%
  filter(n > 4000) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_bar(stat = "identity") +
  xlab(NULL) +
  coord_flip()

#word cloud

#install.packages("wordcloud")
library(wordcloud)

tidy_text %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 200))

#colorcoded word cloud

library(reshape2)
tidy_text %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("#0000FF", "#FF0000"),
                   max.words = 100)

## Repeating the steps of unnesting, stemming on the answers column

##Tokenzing

#unnest_tokens above splits each row such that 
#there is one token (word) in each row of the new data frame

tidy_answer <- data %>%
  unnest_tokens(word, answers)

tidy_answer[1:20]

##Remove StopWords

data(stop_words)

tidy_answer <- tidy_answer %>%
  anti_join(stop_words)

##Count

tidy_answer %>%
  count(word, sort = TRUE)

##Visualization that shows the frequency of the tokens that appeared for at least 2000 times. 

tidy_answer %>%
  count(word, sort = TRUE) %>%
  filter(n > 4000) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_bar(stat = "identity") +
  xlab(NULL) +
  coord_flip()

#stemming using SnowballC

tidy_answer <- data %>%
  unnest_tokens(word, answers) %>%
  mutate(word = wordStem(word)) 

#Removing stopwords

tidy_answer <- tidy_answer %>% 
  anti_join(stop_words)

#count after removing
tidy_answer %>%
  count(word, sort = TRUE)

#Plot a visualization that shows the frequency of the 
# tokens that appeared for at least 6000 times

tidy_answer %>%
  count(word, sort = TRUE) %>%
  filter(n > 6000) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_bar(stat = "identity") +
  xlab(NULL) +
  coord_flip()

#word cloud

tidy_answer %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 200))

#colorcoded word cloud

tidy_answer %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("#0000FF", "#FF0000"),
                   max.words = 100)

#Topic modeling on q_content


data <- data[1:1000,] # We perform LDA on the rows 1 through 1000 in the data.

wordstoremove <- c("know","like","want","dont", "just","feel","get","really","get","said","time","told","can","one","now","always","ive","even")
data_sub <- as.data.frame(sapply(data, function(x) gsub(paste(wordstoremove, collapse = '|'), '', x)))

corpus <- Corpus(VectorSource(data_sub$q_content), readerControl=list(language="en"))
dtm <- DocumentTermMatrix(corpus, control = list(stopwords = TRUE, minWordLength = 2, removeNumbers = TRUE, removePunctuation = TRUE,  stemDocument = TRUE))
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document
dtm.new   <- dtm[rowTotals> 0, ] #remove all docs without words
lda <- LDA(dtm.new, k = 5) # k is the number of topics to be found.

lda_td <- tidy(lda)

#lda_td

#Visualization to find the 10 terms that are most common within each topic

top_terms <- lda_td %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

#Determine k by tuning

#lda <- LDA(dtm.new, k = 5)
library(ldatuning)
tunes <- FindTopicsNumber(
  dtm.new,
  topics = c(2:20),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  mc.cores = 3L,
  verbose = TRUE
)

FindTopicsNumber_plot(tunes)

#k = 2 

lda <- LDA(dtm.new, k = 2) # k is the number of topics to be found.


#Using the tidytext package for extracting the per-topic-per-word probabilities
lda_td <- tidy(lda)
lda_td

#Visualization to find the 10 terms that are most common within each topic

top_terms <- lda_td %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

#k = 3

lda <- LDA(dtm.new, k = 3) # k is the number of topics to be found.


#Using the tidytext package for extracting the per-topic-per-word probabilities
lda_td <- tidy(lda)
lda_td

#Visualization to find the 10 terms that are most common within each topic

top_terms <- lda_td %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

#k=4

lda <- LDA(dtm.new, k = 4) # k is the number of topics to be found.


#Using the tidytext package for extracting the per-topic-per-word probabilities
lda_td <- tidy(lda)
lda_td

#Visualization to find the 10 terms that are most common within each topic

top_terms <- lda_td %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

#k=10

lda <- LDA(dtm.new, k = 10) # k is the number of topics to be found.

#Using the tidytext package for extracting the per-topic-per-word probabilities
lda_td <- tidy(lda)
lda_td

#Visualization to find the 10 terms that are most common within each topic

top_terms <- lda_td %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

#Topic Modeling on Answers

data <- data[1:1000,] # We perform LDA on the rows 1 through 1000 in the data.
corpus <- Corpus(VectorSource(data$answers), readerControl=list(language="en"))
dtm <- DocumentTermMatrix(corpus, control = list(stopwords = TRUE, minWordLength = 2, removeNumbers = TRUE, removePunctuation = TRUE,  stemDocument = TRUE))
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document
dtm.new   <- dtm[rowTotals> 0, ] #remove all docs without words
lda <- LDA(dtm.new, k = 10) # k is the number of topics to be found.

#Using the tidytext package for extracting the per-topic-per-word probabilities
lda_td <- tidy(lda)
lda_td

#Visualization to find the 10 terms that are most common within each topic

top_terms <- lda_td %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

tunes <- FindTopicsNumber(
  dtm.new,
  topics = c(2:30),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  mc.cores = 3L,
  verbose = TRUE
)

FindTopicsNumber_plot(tunes)

#k = 2

lda <- LDA(dtm.new, k = 2) # k is the number of topics to be found.

#Using the tidytext package for extracting the per-topic-per-word probabilities
lda_td <- tidy(lda)
lda_td

#Visualization to find the 10 terms that are most common within each topic

top_terms <- lda_td %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

# k = 8

lda <- LDA(dtm.new, k = 8) # k is the number of topics to be found.

#Using the tidytext package for extracting the per-topic-per-word probabilities
lda_td <- tidy(lda)
lda_td

#Visualization to find the 10 terms that are most common within each topic

top_terms <- lda_td %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

# k = 11

lda <- LDA(dtm.new, k = 11) # k is the number of topics to be found.

#Using the tidytext package for extracting the per-topic-per-word probabilities
lda_td <- tidy(lda)
lda_td

#Visualization to find the 10 terms that are most common within each topic

top_terms <- lda_td %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

# k = 14

lda <- LDA(dtm.new, k = 14) # k is the number of topics to be found.

#Using the tidytext package for extracting the per-topic-per-word probabilities
lda_td <- tidy(lda)
lda_td

#Visualization to find the 10 terms that are most common within each topic

top_terms <- lda_td %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
